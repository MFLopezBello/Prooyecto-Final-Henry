{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo MACHINE LEARNING\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adecuamos dataset de volúmen de tránsito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo CSV en un DataFrame de pandas\n",
    "df_tvc = pd.read_csv('4. Traffic_Volume_Counts_v4_FINAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciona las columnas a partir de \"Date\" en adelante y excluye \"Latitude\" y \"Longitude\"\n",
    "columnas_a_derretir = df_tvc.iloc[:, 6:-3]\n",
    "\n",
    "# Derretir las columnas en filas\n",
    "df_tvc = pd.melt(df_tvc, id_vars=['Date', 'location_id'], \n",
    "                    value_vars=columnas_a_derretir.columns, \n",
    "                    var_name='Hour', value_name='Volume')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un diccionario para mapear los nombres de las columnas actuales a los nombres deseados\n",
    "nuevos_nombres = {\n",
    "    '12:00-1:00 AM': 0,\n",
    "    '1:00-2:00AM': 1,\n",
    "    '2:00-3:00AM': 2,\n",
    "    '3:00-4:00AM': 3,\n",
    "    '4:00-5:00AM': 4,\n",
    "    '5:00-6:00AM': 5,\n",
    "    '6:00-7:00AM': 6,\n",
    "    '7:00-8:00AM': 7,\n",
    "    '8:00-9:00AM': 8,\n",
    "    '9:00-10:00AM': 9,\n",
    "    '10:00-11:00AM': 10,\n",
    "    '11:00-12:00PM': 11,\n",
    "    '12:00-1:00PM': 12,\n",
    "    '1:00-2:00PM': 13,\n",
    "    '2:00-3:00PM': 14,\n",
    "    '3:00-4:00PM': 15,\n",
    "    '4:00-5:00PM': 16,\n",
    "    '5:00-6:00PM': 17,\n",
    "    '6:00-7:00PM': 18,\n",
    "    '7:00-8:00PM': 19,\n",
    "    '8:00-9:00PM': 20,\n",
    "    '9:00-10:00PM': 21,\n",
    "    '10:00-11:00PM': 22,\n",
    "    '11:00-12:00AM': 23\n",
    "}\n",
    "\n",
    "# Cambiar los nombres de las columnas 'Hour' según el diccionario\n",
    "df_tvc['Hour'].replace(nuevos_nombres, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los registros donde 'location_id' es igual a 'No encontrado'\n",
    "df_tvc = df_tvc.loc[df_tvc['location_id'] != 'No encontrado']\n",
    "\n",
    "# Convertir la columna 'Date' a tipo datetime\n",
    "df_tvc['Date'] = pd.to_datetime(df_tvc['Date'], errors='coerce')\n",
    "df_tvc.loc[:, 'Date'] = pd.to_datetime(df_tvc['Date'], errors='coerce')\n",
    "\n",
    "# Extraer los componentes de fecha\n",
    "df_tvc.loc[:, 'day_week'] = df_tvc['Date'].dt.dayofweek\n",
    "\n",
    "# Crear la nueva columna con la combinación de 'day', 'day_week', 'Hour' y 'location_id'\n",
    "df_tvc.loc[:, 'combined_column'] = df_tvc['day_week'].astype(str) + '-' + df_tvc['Hour'].astype(str) + '-' + df_tvc['location_id'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adecuamos dataset de taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo CSV en un DataFrame de pandas\n",
    "df_yellow = pd.read_csv('yellowpreliminar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la primera columna\n",
    "df_yellow.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Renombrar las columnas\n",
    "df_yellow.rename(columns={'pickup_datetime': 'Date',\n",
    "                          'Year': 'year',\n",
    "                          'Month': 'month',\n",
    "                          'Day': 'day',\n",
    "                          'DayofWeek': 'day_week',\n",
    "                          'PULocationID': 'location_id'}, inplace=True)\n",
    "\n",
    "# Eliminar la columna 'pickup_borough'\n",
    "df_yellow.drop(columns=['pickup_borough'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la nueva columna con la combinación de 'day', 'day_week', 'Hour' y 'location_id'\n",
    "df_yellow['combined_column'] = df_yellow['day_week'].astype(str) + '-' + df_yellow['Hour'].astype(str) + '-' + df_yellow['location_id'].astype(str)\n",
    "\n",
    "# Realizar un agrupamiento y sumar la cantidad de registros\n",
    "df_new = df_yellow.groupby('combined_column').size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unificamos los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar un merge entre df_vol y df_new utilizando combined_column como clave\n",
    "df_merged = df_tvc.merge(df_new, on='combined_column', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los NaN de df_merged\n",
    "df_filtered = df_merged.dropna()\n",
    "\n",
    "# Filtrar df_ml para eliminar registros con volume igual a 0\n",
    "df_filtered = df_filtered[df_filtered['Volume'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se realiza cálculo de optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula la optimización de volume vs count\n",
    "df_filtered['optimization'] = df_filtered['count'] / (np.log(df_filtered['Volume'] + 1)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = df_filtered.drop(columns=['Date', 'combined_column'])\n",
    "\n",
    "# Definir las franjas horarias\n",
    "def get_hour_franja(hour):\n",
    "    if 0 <= hour < 6:\n",
    "        return 1  # Franja horaria 1\n",
    "    elif 6 <= hour < 12:\n",
    "        return 2  # Franja horaria 2\n",
    "    elif 12 <= hour < 18:\n",
    "        return 3  # Franja horaria 3\n",
    "    else:\n",
    "        return 4  # Franja horaria 4\n",
    "\n",
    "# Aplicar la función a df_ml para crear una nueva columna 'Hour_franja'\n",
    "df_ml['Hour_franja'] = df_ml['Hour'].apply(get_hour_franja)\n",
    "\n",
    "# Ahora puedes utilizar 'Hour_franja' en lugar de 'Hour' para entrenar el modelo\n",
    "X = df_ml[['day_week', 'Hour_franja', 'location_id']]\n",
    "y = df_ml['optimization']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(df_ml):\n",
    "    # Seleccionar características de entrada (X) y variable objetivo (y)\n",
    "    X = df_ml[['day_week', 'Hour', 'location_id']]\n",
    "    y = df_ml['optimization']\n",
    "\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Inicializar el modelo de Random Forests\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Entrenar el modelo con los datos de entrenamiento\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluar el modelo con los datos de prueba\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Mostrar las métricas de evaluación\n",
    "    print(f\"Error cuadrático medio (MSE): {mse}\")\n",
    "    print(f\"Coeficiente de determinación (R^2): {r2}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_5_location_id(date, hour, model, df_ml):\n",
    "    # Convertir la fecha a day_week\n",
    "    date = pd.to_datetime(date)\n",
    "    day_week = date.dayofweek\n",
    "\n",
    "    # Crear un DataFrame con todas las combinaciones de location_id, day_week, y Hour\n",
    "    unique_locations = df_ml['location_id'].unique()\n",
    "    test_data = pd.DataFrame({\n",
    "        'location_id': unique_locations,\n",
    "        'day_week': [day_week] * len(unique_locations),\n",
    "        'Hour': [hour] * len(unique_locations)\n",
    "    })\n",
    "\n",
    "    test_data = test_data[['day_week', 'Hour', 'location_id']]\n",
    "\n",
    "    # Predecir el valor de optimization para todas las combinaciones\n",
    "    predictions = model.predict(test_data)\n",
    "\n",
    "    # Añadir las predicciones al DataFrame\n",
    "    test_data['optimization'] = predictions\n",
    "\n",
    "    # Ordenar el DataFrame por la columna optimization en orden descendente\n",
    "    test_data_sorted = test_data.sort_values(by='optimization', ascending=False)\n",
    "\n",
    "    # Seleccionar los 5 mejores location_id\n",
    "    top_5_locations = test_data_sorted.head(5)['location_id']\n",
    "\n",
    "    # Retornar los 5 mejores location_id\n",
    "    return list(top_5_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamos modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error cuadrático medio (MSE): 12.152064629818092\n",
      "Coeficiente de determinación (R^2): 0.983214951379792\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo de Random Forests con df_ml\n",
    "model = train_random_forest(df_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 location_id: ['132', '48', '68', '230', '79']\n"
     ]
    }
   ],
   "source": [
    "date = \"2024-01-01\"  # Fecha de ejemplo\n",
    "hour = 5  # Hora de ejemplo\n",
    "\n",
    "top_5_locations = get_top_5_location_id(date, hour, model, df_ml)\n",
    "print(\"Top 5 location_id:\", top_5_locations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
